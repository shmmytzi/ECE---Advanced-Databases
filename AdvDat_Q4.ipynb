{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abf6ef-743f-43fc-9d78-4d1554e894e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The Spark session does not have enough YARN resources to start. \n"
     ]
    }
   ],
   "source": [
    "# QUERY 4 - DataFrame API \n",
    "# 1 core / 2GB\n",
    "\n",
    "import time\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 4 execution\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Register Sedona\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Selected Communities from Query 3\n",
    "communities_max_values = [\"Marina Peninsula\",\"Pacific Palisades\",\"Palisades Highlands\"]\n",
    "communities_min_values = [\"Central\",\"Vernon Central\",\"University Park\"]\n",
    "\n",
    "# Read the file from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "communities_filtered_max_df = blocks_df.select( \\\n",
    "                                           [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                                            blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    ".drop(\"properties\") \\\n",
    ".drop(\"type\")\\\n",
    ".filter(F.col(\"COMM\").isin(communities_max_values))\n",
    "\n",
    "communities_filtered_min_df = blocks_df.select( \\\n",
    "                                           [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                                            blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    ".drop(\"properties\") \\\n",
    ".drop(\"type\")\\\n",
    ".filter(F.col(\"COMM\").isin(communities_min_values))\n",
    "\n",
    "# Read the Race and Ethnicity codes from S3\n",
    "\n",
    "races_schema = StructType([\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Vict Descent Full\", StringType())\n",
    "])\n",
    "    \n",
    "races_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", \n",
    "                           header=True, schema=races_schema)\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", DoubleType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", DoubleType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Read the crime data from CSV files\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721//CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "                           header=True, schema=crime_schema)\n",
    "\n",
    "crime_filtered_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").filter(\n",
    "    F.col(\"LAT\").isNotNull() & \n",
    "    F.col(\"LON\").isNotNull() & \n",
    "    (F.col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    ").withColumn(\"geometry\", ST_Point(F.col(\"LON\"), F.col(\"LAT\")))\n",
    "\n",
    "output_max_df = crime_filtered_df\\\n",
    ".join(communities_filtered_max_df, ST_Within(crime_filtered_df.geometry, communities_filtered_max_df.geometry), \"inner\")\\\n",
    ".select(\"Vict Descent\")\n",
    "\n",
    "output_min_df = crime_filtered_df\\\n",
    ".join(communities_filtered_min_df, ST_Within(crime_filtered_df.geometry, communities_filtered_min_df.geometry), \"inner\")\\\n",
    ".select(\"Vict Descent\")\n",
    "\n",
    "output_adjusted_max_df = output_max_df.groupBy(\"Vict Descent\").agg(\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "output_adjusted_min_df = output_min_df.groupBy(\"Vict Descent\").agg(\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "output_final_max_df = output_adjusted_max_df.join(\n",
    "    races_df,\n",
    "    on=\"Vict Descent\", \n",
    "    how=\"left\"\n",
    ").select(\"Vict Descent Full\", \"#\").orderBy(col(\"#\").desc()).withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "\n",
    "output_final_min_df = output_adjusted_min_df.join(\n",
    "    races_df,\n",
    "    on=\"Vict Descent\", \n",
    "    how=\"left\"\n",
    ").select(\"Vict Descent Full\", \"#\").orderBy(col(\"#\").desc()).withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "\n",
    "output_final_max_df.show()\n",
    "output_final_min_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cb604-2b73-49eb-8e55-c041da673348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUERY 4 - DataFrame API \n",
    "# 2 cores / 4GB\n",
    "\n",
    "import time\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 4 execution\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Register Sedona\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Selected Communities from Query 3\n",
    "communities_max_values = [\"Marina Peninsula\",\"Pacific Palisades\",\"Palisades Highlands\"]\n",
    "communities_min_values = [\"Central\",\"Vernon Central\",\"University Park\"]\n",
    "\n",
    "# Read the file from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "communities_filtered_max_df = blocks_df.select( \\\n",
    "                                           [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                                            blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    ".drop(\"properties\") \\\n",
    ".drop(\"type\")\\\n",
    ".filter(F.col(\"COMM\").isin(communities_max_values))\n",
    "\n",
    "communities_filtered_min_df = blocks_df.select( \\\n",
    "                                           [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                                            blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    ".drop(\"properties\") \\\n",
    ".drop(\"type\")\\\n",
    ".filter(F.col(\"COMM\").isin(communities_min_values))\n",
    "\n",
    "# Read the Race and Ethnicity codes from S3\n",
    "\n",
    "races_schema = StructType([\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Vict Descent Full\", StringType())\n",
    "])\n",
    "    \n",
    "races_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", \n",
    "                           header=True, schema=races_schema)\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", DoubleType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", DoubleType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Read the crime data from CSV files\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721//CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "                           header=True, schema=crime_schema)\n",
    "\n",
    "crime_filtered_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").filter(\n",
    "    F.col(\"LAT\").isNotNull() & \n",
    "    F.col(\"LON\").isNotNull() & \n",
    "    (F.col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    ").withColumn(\"geometry\", ST_Point(F.col(\"LON\"), F.col(\"LAT\")))\n",
    "\n",
    "output_max_df = crime_filtered_df\\\n",
    ".join(communities_filtered_max_df, ST_Within(crime_filtered_df.geometry, communities_filtered_max_df.geometry), \"inner\")\\\n",
    ".select(\"Vict Descent\")\n",
    "\n",
    "output_min_df = crime_filtered_df\\\n",
    ".join(communities_filtered_min_df, ST_Within(crime_filtered_df.geometry, communities_filtered_min_df.geometry), \"inner\")\\\n",
    ".select(\"Vict Descent\")\n",
    "\n",
    "output_adjusted_max_df = output_max_df.groupBy(\"Vict Descent\").agg(\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "output_adjusted_min_df = output_min_df.groupBy(\"Vict Descent\").agg(\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "output_final_max_df = output_adjusted_max_df.join(\n",
    "    races_df,\n",
    "    on=\"Vict Descent\", \n",
    "    how=\"left\"\n",
    ").select(\"Vict Descent Full\", \"#\").orderBy(col(\"#\").desc()).withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "\n",
    "output_final_min_df = output_adjusted_min_df.join(\n",
    "    races_df,\n",
    "    on=\"Vict Descent\", \n",
    "    how=\"left\"\n",
    ").select(\"Vict Descent Full\", \"#\").orderBy(col(\"#\").desc()).withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "\n",
    "output_final_max_df.show()\n",
    "output_final_min_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0b20d-bbbd-417c-92e7-cac69f8b31f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUERY 4 - DataFrame API \n",
    "# 4 cores / 8GB\n",
    "\n",
    "import time\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 4 execution\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Sedona\n",
    "sedona = SedonaContext.create(spark)\n",
    "# Register Sedona\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Selected Communities from Query 3\n",
    "communities_max_values = [\"Marina Peninsula\",\"Pacific Palisades\",\"Palisades Highlands\"]\n",
    "communities_min_values = [\"Central\",\"Vernon Central\",\"University Park\"]\n",
    "\n",
    "# Read the file from S3\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "\n",
    "blocks_df = sedona.read.format(\"geojson\") \\\n",
    "    .option(\"multiLine\", \"true\").load(geojson_path) \\\n",
    "    .selectExpr(\"explode(features) as features\") \\\n",
    "    .select(\"features.*\")\n",
    "\n",
    "communities_filtered_max_df = blocks_df.select( \\\n",
    "                                           [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                                            blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    ".drop(\"properties\") \\\n",
    ".drop(\"type\")\\\n",
    ".filter(F.col(\"COMM\").isin(communities_max_values))\n",
    "\n",
    "communities_filtered_min_df = blocks_df.select( \\\n",
    "                                           [F.col(f\"properties.{col_name}\").alias(col_name) for col_name in \\\n",
    "                                            blocks_df.schema[\"properties\"].dataType.fieldNames()] + [\"geometry\"]) \\\n",
    ".drop(\"properties\") \\\n",
    ".drop(\"type\")\\\n",
    ".filter(F.col(\"COMM\").isin(communities_min_values))\n",
    "\n",
    "# Read the Race and Ethnicity codes from S3\n",
    "\n",
    "races_schema = StructType([\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Vict Descent Full\", StringType())\n",
    "])\n",
    "    \n",
    "races_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", \n",
    "                           header=True, schema=races_schema)\n",
    "\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", DoubleType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", DoubleType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "# Read the crime data from CSV files\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721//CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "                           header=True, schema=crime_schema)\n",
    "\n",
    "crime_filtered_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").filter(\n",
    "    F.col(\"LAT\").isNotNull() & \n",
    "    F.col(\"LON\").isNotNull() & \n",
    "    (F.col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    ").withColumn(\"geometry\", ST_Point(F.col(\"LON\"), F.col(\"LAT\")))\n",
    "\n",
    "output_max_df = crime_filtered_df\\\n",
    ".join(communities_filtered_max_df, ST_Within(crime_filtered_df.geometry, communities_filtered_max_df.geometry), \"inner\")\\\n",
    ".select(\"Vict Descent\")\n",
    "\n",
    "output_min_df = crime_filtered_df\\\n",
    ".join(communities_filtered_min_df, ST_Within(crime_filtered_df.geometry, communities_filtered_min_df.geometry), \"inner\")\\\n",
    ".select(\"Vict Descent\")\n",
    "\n",
    "output_adjusted_max_df = output_max_df.groupBy(\"Vict Descent\").agg(\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "output_adjusted_min_df = output_min_df.groupBy(\"Vict Descent\").agg(\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "output_final_max_df = output_adjusted_max_df.join(\n",
    "    races_df,\n",
    "    on=\"Vict Descent\", \n",
    "    how=\"left\"\n",
    ").select(\"Vict Descent Full\", \"#\").orderBy(col(\"#\").desc()).withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "\n",
    "output_final_min_df = output_adjusted_min_df.join(\n",
    "    races_df,\n",
    "    on=\"Vict Descent\", \n",
    "    how=\"left\"\n",
    ").select(\"Vict Descent Full\", \"#\").orderBy(col(\"#\").desc()).withColumnRenamed(\"Vict Descent Full\", \"Vict Descent\")\n",
    "\n",
    "output_final_max_df.show()\n",
    "output_final_min_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
