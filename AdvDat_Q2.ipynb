{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e650de3-3fc8-4590-a689-ecbc1ce4373c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUERY 2 - DataFrame API\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the crime data\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", DoubleType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", DoubleType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "\n",
    "def calculate_percentage(closed_cases, total_cases):\n",
    "    if total_cases == 0:\n",
    "        return 0.0\n",
    "    return ((closed_cases / total_cases) * 100)\n",
    "\n",
    "\n",
    "# Register\n",
    "calculate_percentage = udf(calculate_percentage, DoubleType())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the crime data from CSV files\n",
    "crime1_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721//CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", \n",
    "                           header=False, schema=crime_schema)\n",
    "crime2_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721//CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\", \n",
    "                           header=False, schema=crime_schema)\n",
    "\n",
    "# Union the dataframes to combine the two datasets\n",
    "crime_df = crime1_df.union(crime2_df)\n",
    "\n",
    "# DATE OCC from StringType() to TimestampType()\n",
    "crime_df_fixed = crime_df.withColumn(\n",
    "    \"DATE OCC\", F.to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# F.year(\"DATE OCC\") extracts the year from the DATE OCC\n",
    "crime_df_adjusted = crime_df_fixed.withColumn(\"year\", F.year(\"DATE OCC\"))\n",
    "\n",
    "crime_df_filtered = crime_df_adjusted.filter(~F.col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"))\n",
    "\n",
    "total_cases = (\n",
    "    crime_df_adjusted.groupBy(\"year\", \"AREA NAME\")\n",
    "    .agg(F.count(\"*\").alias(\"TOTAL CASES\")))\n",
    "\n",
    "closed_cases = (\n",
    "    crime_df_filtered.groupBy(\"year\", \"AREA NAME\")\n",
    "    .agg(F.count(\"*\").alias(\"CLOSED CASES\")))\n",
    "\n",
    "\n",
    "percentage = closed_cases.join(total_cases, [\"year\", \"AREA NAME\"]) \\\n",
    "    .withColumn(\"closed_case_rate\",\n",
    "                calculate_percentage(F.col(\"CLOSED CASES\"),\n",
    "                                     F.col(\"TOTAL CASES\")))\n",
    "\n",
    "crime_df_ranked = percentage.withColumn(\"#\", F.row_number().over(\n",
    "    Window.partitionBy(\"year\").orderBy(F.col(\"closed_case_rate\").desc())))\n",
    "\n",
    "crime_df_ranked_top3 = crime_df_ranked.filter(F.col(\"#\") <= 3)\n",
    "\n",
    "crime_df_ranked_top3_output = crime_df_ranked_top3.select(\"year\", \"AREA NAME\",\n",
    "                                                          \"closed_case_rate\", \"#\")\n",
    "\n",
    "# Show the result ordered by Year and Ranking\n",
    "crime_df_ranked_top3_output.orderBy(\"year\", \"#\").show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97697d-5a8b-48bc-a426-5b1e1b15ff81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUERY 2 - Spark SQL\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SQL query 2 execution\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the crime data\n",
    "crime_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"Date Rptd\", StringType()),\n",
    "    StructField(\"DATE OCC\", StringType()),\n",
    "    StructField(\"TIME OCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREA NAME\", StringType()),\n",
    "    StructField(\"Rpt Dist No\", StringType()),\n",
    "    StructField(\"Part 1-2\", DoubleType()),\n",
    "    StructField(\"Crm Cd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"Vict Sex\", StringType()),\n",
    "    StructField(\"Vict Descent\", StringType()),\n",
    "    StructField(\"Premis Cd\", DoubleType()),\n",
    "    StructField(\"Premis Desc\", StringType()),\n",
    "    StructField(\"Weapon Used Cd\", StringType()),\n",
    "    StructField(\"Weapon Desc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"Crm Cd 1\", StringType()),\n",
    "    StructField(\"Crm Cd 2\", StringType()),\n",
    "    StructField(\"Crm Cd 3\", StringType()),\n",
    "    StructField(\"Crm Cd 4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"Cross Street\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType())\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the crime data from CSV files\n",
    "crime_1 = spark.read.format('csv') \\\n",
    "                    .options(header='false') \\\n",
    "                    .schema(crime_schema) \\\n",
    "                    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721//CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "crime_2 = spark.read.format('csv') \\\n",
    "                    .options(header='false') \\\n",
    "                    .schema(crime_schema) \\\n",
    "                    .load(\"s3://initial-notebook-data-bucket-dblab-905418150721//CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\")\n",
    "\n",
    "# To utilize as SQL tables\n",
    "crime_1.createOrReplaceTempView(\"crime_1\")\n",
    "crime_2.createOrReplaceTempView(\"crime_2\")\n",
    "\n",
    "crime_df = spark.sql(\"\"\"\n",
    "SELECT * FROM crime_1\n",
    "UNION\n",
    "SELECT * FROM crime_2\n",
    "\"\"\")\n",
    "\n",
    "# To utilize as SQL table\n",
    "crime_df.createOrReplaceTempView(\"crime_df\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *,\n",
    "       TO_TIMESTAMP(`DATE OCC`, 'MM/dd/yyyy hh:mm:ss a') AS `DATE OCC TIMESTAMP`,\n",
    "       YEAR(TO_TIMESTAMP(`DATE OCC`, 'MM/dd/yyyy hh:mm:ss a')) AS `year`\n",
    "FROM crime_df\n",
    "\"\"\"\n",
    "\n",
    "crime_df_adjusted = spark.sql(query)\n",
    "# To utilize as SQL table\n",
    "crime_df_adjusted.createOrReplaceTempView(\"crime_df_adjusted\")\n",
    "\n",
    "query = \"\"\"\n",
    "WITH total_cases AS (\n",
    "    SELECT `year`, `AREA NAME`, COUNT(*) AS TOTAL_CASES\n",
    "    FROM crime_df_adjusted\n",
    "    WHERE `year` IS NOT NULL\n",
    "    GROUP BY `year`, `AREA NAME`\n",
    "),\n",
    "closed_cases AS (\n",
    "    SELECT `year`, `AREA NAME`, COUNT(*) AS CLOSED_CASES\n",
    "    FROM crime_df_adjusted\n",
    "    WHERE `Status Desc` NOT IN ('UNK', 'Invest Cont')\n",
    "    GROUP BY `year`, `AREA NAME`\n",
    "),\n",
    "percentage AS (\n",
    "    SELECT t.`year`, t.`AREA NAME`, t.TOTAL_CASES, c.CLOSED_CASES,\n",
    "        CASE WHEN t.TOTAL_CASES = 0 THEN 0 \n",
    "        ELSE (c.CLOSED_CASES / t.TOTAL_CASES) * 100 END AS closed_case_rate\n",
    "    FROM total_cases t\n",
    "    LEFT JOIN closed_cases c\n",
    "    ON t.`year` = c.`year` AND t.`AREA NAME` = c.`AREA NAME`\n",
    "),\n",
    "ranked_percentage AS (\n",
    "    SELECT `year`, `AREA NAME`, closed_case_rate,\n",
    "        ROW_NUMBER() OVER (PARTITION BY `year` ORDER BY closed_case_rate DESC) AS rank\n",
    "    FROM percentage\n",
    ")\n",
    "SELECT `year`, `AREA NAME`, closed_case_rate, rank\n",
    "FROM ranked_percentage\n",
    "WHERE rank <= 3\n",
    "ORDER BY `year`, rank;\n",
    "\"\"\"\n",
    "\n",
    "crime_df_ranked_top3_output = spark.sql(query)\n",
    "\n",
    "crime_df_ranked_top3_output.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a776c-dedb-4c98-8a33-3b7608384284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUERY 2 - parquet\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"el21103@mail.ntua.gr\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"BVwyriNr@Pt^G&PVex9}\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "path = \"s3://groups-bucket-dblab-905418150721/group42/crime_data.parquet\"\n",
    "\n",
    "crime_df.coalesce(1).write.mode(\"overwrite\").parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52378af2-ab5f-4de6-a314-a7ab6537cc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUERY 2 - DataFrame API - parquet\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DF query 2 execution with Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a UDF to calculate the percentage of closed cases\n",
    "def calculate_percentage(closed_cases, total_cases):\n",
    "    if total_cases == 0:\n",
    "        return 0.0\n",
    "    return ((closed_cases / total_cases) * 100)\n",
    "\n",
    "# Register\n",
    "calculate_percentage = udf(calculate_percentage, DoubleType())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "crime_df = spark.read.parquet(\"s3://groups-bucket-dblab-905418150721/group42/crime_data.parquet\")\n",
    "\n",
    "# Ensure DATE OCC is in TimestampType\n",
    "crime_df_fixed = crime_df.withColumn(\n",
    "    \"DATE OCC\", F.to_timestamp(\"DATE OCC\", \"MM/dd/yyyy hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "\n",
    "crime_df_adjusted = crime_df_fixed.withColumn(\"year\", F.year(\"DATE OCC\"))\n",
    "\n",
    "\n",
    "crime_df_filtered = crime_df_adjusted.filter(~F.col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"))\n",
    "\n",
    "\n",
    "total_cases = (\n",
    "    crime_df_adjusted.groupBy(\"year\", \"AREA NAME\")\n",
    "    .agg(F.count(\"*\").alias(\"TOTAL CASES\"))\n",
    ")\n",
    "\n",
    "\n",
    "closed_cases = (\n",
    "    crime_df_filtered.groupBy(\"year\", \"AREA NAME\")\n",
    "    .agg(F.count(\"*\").alias(\"CLOSED CASES\"))\n",
    ")\n",
    "\n",
    "\n",
    "percentage = closed_cases.join(total_cases, [\"year\", \"AREA NAME\"]) \\\n",
    "    .withColumn(\"closed_case_rate\", \n",
    "                calculate_percentage(F.col(\"CLOSED CASES\"), \n",
    "                                     F.col(\"TOTAL CASES\")))\n",
    "\n",
    "\n",
    "crime_df_ranked = percentage.withColumn(\"#\", F.row_number().over(\n",
    "    Window.partitionBy(\"year\").orderBy(F.col(\"closed_case_rate\").desc())))\n",
    "\n",
    "\n",
    "crime_df_ranked_top3 = crime_df_ranked.filter(F.col(\"#\") <= 3)\n",
    "\n",
    "# Select the required columns for output\n",
    "crime_df_ranked_top3_output = crime_df_ranked_top3.select(\"year\", \"AREA NAME\", \n",
    "                                                          \"closed_case_rate\", \"#\")\n",
    "\n",
    "\n",
    "crime_df_ranked_top3_output.orderBy(\"year\", \"#\").show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
